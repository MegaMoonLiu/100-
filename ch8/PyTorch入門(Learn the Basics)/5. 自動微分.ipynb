{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4","toc_visible":true,"authorship_tag":"ABX9TyNrkIcdLzPng7ZX036A/8Iv"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# **「PyTorch入門 5. 自動微分」**"],"metadata":{"id":"J2oMDT_o-D3o"}},{"cell_type":"markdown","source":["## **Automatic Differentiation with torch.autograd**(通过torch.autograd自动微分)\n","\n","在训练神经网络时，作为一种学习算法，被用作基本的反向传播(back propagation)\n","\n","反向传播(back propagation)对于模型中各个参数如权重，损失函数与变量的微分值进行调整\n","\n","在PyTorch中，通过``torch.autograd``计算梯度"],"metadata":{"id":"cHtHGvEE-HZb"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZpfDevTW9-wH"},"outputs":[],"source":["%matplotlib inline"]},{"cell_type":"code","source":["import torch\n","\n","x = torch.ones(5)\n","# input 输入张量\n","y = torch.zeros(3)\n","# output 预期输出\n","\n","w = torch.randn(5, 3, requires_grad=True)\n","b = torch.randn(3, requires_grad=True)\n","# requires_grad=True表示需要计算梯度\n","\n","z = torch.matmul(x, w)+b\n","# z = x,w做矩阵乘法 + b\n","\n","# 计算loss\n","# 二元交叉熵损失\n","loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)\n","print(loss)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f9bZ-sAU_T5M","executionInfo":{"status":"ok","timestamp":1717316816109,"user_tz":-540,"elapsed":2,"user":{"displayName":"Moon Air","userId":"01277311785671261049"}},"outputId":"4ac909f2-d841-491b-e06f-be93dcf06cb7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(0.7341, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"]}]},{"cell_type":"markdown","source":["## **テンソル、関数、計算グラフの関係**(张量，函数，计算图的关系)\n","\n","<img src=\"https://pytorch.org/tutorials/_images/comp-graph.png\" width=50%>\n","\n","为了找到最适w b 需要用到loss function\n","\n","requires_grad=True表示需要计算梯度\n","\n","【注意】\n","\n","在构建计算图时，应用于张量的函数实际上是函数类的对象。\n","\n","这些对象定义了在前向传播过程中如何处理输入。\n","\n","此外，它们还知道如何在反向传播过程中计算梯度。\n","\n","\n","梯度将存储在张量的 grad_fn 属性中。"],"metadata":{"id":"PHlyCmN_AQS6"}},{"cell_type":"code","source":["print('Gradient function for z =',z.grad_fn)\n","print('Gradient function for loss =', loss.grad_fn)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0xQIa2ujBNKJ","executionInfo":{"status":"ok","timestamp":1717316459335,"user_tz":-540,"elapsed":617,"user":{"displayName":"Moon Air","userId":"01277311785671261049"}},"outputId":"2e83de2d-e1da-43cf-d0c7-f546f8da84b8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Gradient function for z = <AddBackward0 object at 0x7d23ee277070>\n","Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward0 object at 0x7d23ee276230>\n"]}]},{"cell_type":"markdown","source":["## **勾配の計算**(梯度的计算)\n","\n","为了使神经网络中的各个参数都最适化，当给定输入``x``和输出``y``时，对loss function中的各个参数进行偏微分\n","\n","$\\frac{\\partial loss}{\\partial w}$ 、$\\frac{\\partial loss}{\\partial b}$\n","\n","为了求出偏微分值，通过``loss.backward()``，求出``w.grad``和``b.grad``"],"metadata":{"id":"EeUAEcZACfrg"}},{"cell_type":"code","source":["loss.backward()\n","print(w.grad)\n","print(b.grad)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"llKYPhsGDjs9","executionInfo":{"status":"ok","timestamp":1717316764729,"user_tz":-540,"elapsed":5,"user":{"displayName":"Moon Air","userId":"01277311785671261049"}},"outputId":"250ece44-13fc-4800-de0f-ea01e49cc630"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.2133, 0.3034, 0.3007],\n","        [0.2133, 0.3034, 0.3007],\n","        [0.2133, 0.3034, 0.3007],\n","        [0.2133, 0.3034, 0.3007],\n","        [0.2133, 0.3034, 0.3007]])\n","tensor([0.2133, 0.3034, 0.3007])\n"]}]},{"cell_type":"markdown","source":["【注意】\n","\n","``grad``是计算图中的叶节点(leaf node)也就是说当且仅当requires_grad=True时才会计算\n","\n","不是所有的函数都可以计算梯度\n","\n","``backward``只运行一次(出于性能考虑)\n","\n","如果想进行多次，则需要在 ``backward` 时将 ``retain_graph=True` 作为参数传递"],"metadata":{"id":"nSJnfyGREj_f"}},{"cell_type":"markdown","source":["## **勾配計算をしない方法**(不进行梯度计算)\n","\n","计算梯度时默认选项\n","\n","当不打算计算梯度时，需确保在 ``torch.no_grad()`` 代码块中包含这些代码"],"metadata":{"id":"Hr3crKaSFhfo"}},{"cell_type":"code","source":["# 计算梯度\n","z = torch.matmul(x, w)+b\n","print(z.requires_grad)\n","\n","# 不计算梯度\n","with torch.no_grad():\n","    z = torch.matmul(x, w)+b\n","print(z.requires_grad)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mIWfgy6oF3GV","executionInfo":{"status":"ok","timestamp":1717317368795,"user_tz":-540,"elapsed":7,"user":{"displayName":"Moon Air","userId":"01277311785671261049"}},"outputId":"d16e23cb-0ec3-4b1c-80e0-99339aea7fbf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["True\n","False\n"]}]},{"cell_type":"markdown","source":["同样可以对张量进行``detch()``来实现不计算梯度"],"metadata":{"id":"Ab-8zaoqGBCf"}},{"cell_type":"code","source":["z = torch.matmul(x, w)+b\n","z_det = z.detach()\n","print(z_det.requires_grad)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W8KWeiURGJW-","executionInfo":{"status":"ok","timestamp":1717317442859,"user_tz":-540,"elapsed":5,"user":{"displayName":"Moon Air","userId":"01277311785671261049"}},"outputId":"a418466e-4052-4bca-e1e4-355fd2345254"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["False\n"]}]},{"cell_type":"markdown","source":["## **計算グラフについて補足**(对计算图的补充)\n","\n","从理论上来说，``autograd``是一种图形，它以有向无环图(Directed Acyclic Graph)的形式存储张量和对张量的运算，并以函数作为构建模块。\n","\n","在 DAG 中，每个叶节点(leaf)都是一个输入张量，根(root)是一个输出张量。\n","\n","利用微分的链式法则，从根(root)追踪到每个叶(leaf)，就能确定每个变量的偏导数值。"],"metadata":{"id":"ripV0c2pIAVI"}},{"cell_type":"markdown","source":["在正向传播中，``autograd``同时执行以下处理\n","\n","\n","*   执行指定操作并获取计算结果的张量\n","*   更新每个 DAG 运算的梯度函数\n","\n","<br>\n","在反向传播中，当DAG根张量执行``.backward()``时，``autograd``执行以下处理\n","\n","\n","\n","*   计算每个变量的 .grad_fn\n","*   为每个变量的 .grad 属性分配导数值\n","*   使用微分链规则计算每个叶子张量的导数值\n","\n","\n","\n"],"metadata":{"id":"XuflvoBzIqsU"}},{"cell_type":"markdown","source":["【注意】\n","\n","在PyTorch中DAG是动态的(在函数计算处理时按照顺序构建)\n","\n","每次调用 ``.backward()``，``autograd`` 都会再次创建一个新的图\n","\n","正是由于这一特性，在模型的前向传播过程中可以使用控制流语句（``if`` 和 ``for`` 语句），从而可以在每次迭代时根据需要改变图形的形状、大小和操作。"],"metadata":{"id":"DbRraW3FJcnJ"}},{"cell_type":"markdown","source":["## **补充**\n","\n","大多数的情况中，对于输出标量的loss function，在计算某个变量的梯度\n","\n","但是，函数的输出不一定是标量，可以是任意的张量\n","\n","在这种情况下，PyTorch计算Jacobian matrix矩阵，而不是梯度"],"metadata":{"id":"VKLVj741J1v6"}},{"cell_type":"markdown","source":["\\begin{split}\\begin{align}J=\\left(\\begin{array}{ccc}\n","   \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\\n","   \\vdots & \\ddots & \\vdots\\\\\n","   \\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n","   \\end{array}\\right)\\end{align}\\end{split}"],"metadata":{"id":"8BGs6Jn0Kn-6"}},{"cell_type":"code","source":["inp = torch.eye(5, requires_grad=True)\n","# 生成一个对角线元素为1，其余元素为0的5*5的矩阵\n","\n","out = (inp+1).pow(2)\n","# inp中的每个元素 +1 & 平方\n","\n","out.backward(torch.ones_like(inp), retain_graph=True)\n","print(\"First call\\n\", inp.grad)\n","# out相对于inp的梯度\n","\n","\n","out.backward(torch.ones_like(inp), retain_graph=True)\n","print(\"\\nSecond call\\n\", inp.grad)\n","# 再次计算梯度\n","\n","inp.grad.zero_()\n","# 清零梯度\n","out.backward(torch.ones_like(inp), retain_graph=True)\n","# 计算清零后的梯度\n","print(\"\\nCall after zeroing gradients\\n\", inp.grad)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9pH88sI5JY_S","executionInfo":{"status":"ok","timestamp":1717318903806,"user_tz":-540,"elapsed":1198,"user":{"displayName":"Moon Air","userId":"01277311785671261049"}},"outputId":"0a781bdb-c265-404e-87a2-57ab8204e6ef"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["First call\n"," tensor([[4., 2., 2., 2., 2.],\n","        [2., 4., 2., 2., 2.],\n","        [2., 2., 4., 2., 2.],\n","        [2., 2., 2., 4., 2.],\n","        [2., 2., 2., 2., 4.]])\n","\n","Second call\n"," tensor([[8., 4., 4., 4., 4.],\n","        [4., 8., 4., 4., 4.],\n","        [4., 4., 8., 4., 4.],\n","        [4., 4., 4., 8., 4.],\n","        [4., 4., 4., 4., 8.]])\n","\n","Call after zeroing gradients\n"," tensor([[4., 2., 2., 2., 2.],\n","        [2., 4., 2., 2., 2.],\n","        [2., 2., 4., 2., 2.],\n","        [2., 2., 2., 4., 2.],\n","        [2., 2., 2., 2., 4.]])\n"]}]}]}