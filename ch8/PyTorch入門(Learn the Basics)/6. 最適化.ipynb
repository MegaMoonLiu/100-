{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNfkd2K3l9a4K68JC+Vih7j"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **「PyTorch入門 6. 最適化」**"],"metadata":{"id":"NciFPeo05RAZ"}},{"cell_type":"markdown","source":["## **パラメータの最適化**(参数的最优化)\n","\n","在每次迭代中，计算model的输出，求出损失，以及对于每个参数关于损失的偏微分。\n","\n","之后，利用梯度下降法做参数的最优化\n","\n"],"metadata":{"id":"jCcBQJC45k9w"}},{"cell_type":"markdown","source":["## **コード準備**(代码的准备)"],"metadata":{"id":"GalBMdfP8GPd"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"nbO9BHO05NZq","executionInfo":{"status":"ok","timestamp":1717566527509,"user_tz":-540,"elapsed":4,"user":{"displayName":"Moon Air","userId":"01277311785671261049"}}},"outputs":[],"source":["%matplotlib inline"]},{"cell_type":"code","source":["import torch\n","from torch import nn\n","from torch.utils.data import DataLoader\n","from torchvision import datasets\n","from torchvision.transforms import ToTensor, Lambda\n","\n","# 下载FashionMNIST数据集\n","# 训练用数据集\n","training_data = datasets.FashionMNIST(\n","    root=\"data\",\n","    train=True,\n","    download=True,\n","    transform=ToTensor()\n",")\n","# 测试用数据集\n","test_data = datasets.FashionMNIST(\n","    root=\"data\",\n","    train=False,\n","    download=True,\n","    transform=ToTensor()\n",")\n","\n","# 加载数据\n","train_dataloader = DataLoader(training_data, batch_size=64)\n","test_dataloader = DataLoader(test_data, batch_size=64)\n","\n","# 构建神经网络\n","class NeuralNetwork(nn.Module):\n","\n","    # 定义__init__()构造函数\n","    def __init__(self):\n","\n","        # 调用父类的构造函数，确保父类nn.Module的初始化也被执行\n","        super(NeuralNetwork, self).__init__()\n","\n","        # 定义了一个扁平化层，将输入的28x28图像转换为784维的一维向量\n","        # 第0维度代表样本编号，该维度不会通过nn.Flatten发生变化\n","        self.flatten = nn.Flatten()\n","\n","\n","        # 定义了一系列线性层和ReLU激活函数组成的顺序容器（sequential container）\n","        self.linear_relu_stack = nn.Sequential(\n","\n","            # 第一个线性层，将输入的784维向量映射到512维\n","            nn.Linear(28*28, 512),\n","            # 一个ReLU激活函数，应用非线性变换\n","            nn.ReLU(),\n","\n","            # 第二个线性层，将512维向量映射到另一个512维向量\n","            nn.Linear(512, 512),\n","            # 再次应用nn.ReLU()\n","            nn.ReLU(),\n","\n","            # 第三个线性层，将512维向量映射到10维向量\n","            nn.Linear(512, 10),\n","            # 再应用一次nn.ReLU()非线性变换。\n","            nn.ReLU()\n","        )\n","\n","    def forward(self, x):\n","\n","        # 扁平化输入\n","        x = self.flatten(x)\n","\n","        # 将扁平化后的向量通过定义的线性和ReLU层的堆栈\n","        logits = self.linear_relu_stack(x)\n","\n","        # 返回最终的输出\n","        return logits\n","\n","# 创建实例\n","model = NeuralNetwork()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4Iq3eyNb8aP4","executionInfo":{"status":"ok","timestamp":1717566707151,"user_tz":-540,"elapsed":13457,"user":{"displayName":"Moon Air","userId":"01277311785671261049"}},"outputId":"2f4d3cee-0644-43b0-aa98-0cb8c03b5a0b"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 26421880/26421880 [00:01<00:00, 13336851.87it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n","\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 29515/29515 [00:00<00:00, 209165.27it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n","\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4422102/4422102 [00:01<00:00, 3878381.10it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n","\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 5148/5148 [00:00<00:00, 14075799.86it/s]"]},{"output_type":"stream","name":"stdout","text":["Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n","\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","source":["## **ハイパーパラメータ**(超参数 Hyperparameter)\n","\n","在机器学习中,超参(数英语:Hyperparameter)是事先给定的,用来控制学习过程的参数。而其他参数(例如节点权重)的值是通过训练得出的.\n","\n","因超参数值的不同,影响model的学习以及收束率."],"metadata":{"id":"ct6VnVcp9v5B"}},{"cell_type":"markdown","source":["本次实验中,使用的超参数如下.\n","* Number of Epochs: 迭代次数\n","* Batch Size: 构成mini-batch的数据数\n","* Learning Rate:参数更新的系数。值越小变化越小、过大时可能会导致训练失败"],"metadata":{"id":"Sc1Xc_XJ_IA0"}},{"cell_type":"code","source":["learning_rate = 1e-3\n","batch_size = 64\n","epochs = 5"],"metadata":{"id":"QgIprZNEAFIc","executionInfo":{"status":"ok","timestamp":1717567511945,"user_tz":-540,"elapsed":5,"user":{"displayName":"Moon Air","userId":"01277311785671261049"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["## **最適化ループ**(最优化循环)\n","\n","设定好超参数后,通过训练进行最优化循环,对model进行最优化\n","\n","最优化的每一次迭代称为一个エポック(纪元 epoch)\n","\n","每个纪元由两种循环构成\n","\n","* 训练循环: 针对dataset进行训练,使得参数收束\n","* 验证/测试循环: 通过test dataset对moddel进行评价,确认是否性能有所提高"],"metadata":{"id":"kgeKC1zRAIKp"}},{"cell_type":"markdown","source":["**損失関数：Loss Function**\n","\n","即便是给定data,未经训练的 network 可能无法输出正确答案。\n","\n","loss function是测定model推断的结果和实际正解之间的误差大小的函数\n","\n","利用不停的训练,使loss function的值变小\n","\n","为了计算损失,需要获取model对输入数据的推断,并比较该值与正确标签之间的差异"],"metadata":{"id":"aJ6NyTkvCwEE"}},{"cell_type":"markdown","source":["一般情况下,对于回归问题使用``nn.MSELoss``(Mean Square Error),分类问题使用``nn.NLLLoss``(Negative Log Likelihood`)\n","\n","``nn.CrossEntropyLoss``是``nn.LogSoftmax`` 和 ``nn.NLLLoss``的结合\n","\n","将model的输出``logit``値输入``nn.CrossEntropyLoss``进行正規化(标准化 Normalization),计算预测误差"],"metadata":{"id":"xPjAQgq9EF1f"}},{"cell_type":"code","source":["# 定义loss function\n","loss_fn = nn.CrossEntropyLoss()"],"metadata":{"id":"VdLkKIU1EFjw","executionInfo":{"status":"ok","timestamp":1717568870485,"user_tz":-540,"elapsed":6,"user":{"displayName":"Moon Air","userId":"01277311785671261049"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["最优化器: Optimizer\n","为了使得各个训练步骤中model的误差变小,对model参数进行调整的进程\n","\n","最优化算法: Optimization algorithms\n","\n","实现最优化的具体步骤\n","\n","最优化的逻辑都在``optimize``内"],"metadata":{"id":"V7WdGJtaGwPU"}},{"cell_type":"markdown","source":["将model的参数,并把学习率作为超参数输入输入optimizer内进行初始化"],"metadata":{"id":"7VXaUrKkHkuQ"}},{"cell_type":"code","source":["optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"],"metadata":{"id":"kIfps-RKIBBb","executionInfo":{"status":"ok","timestamp":1717569591456,"user_tz":-540,"elapsed":290,"user":{"displayName":"Moon Air","userId":"01277311785671261049"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["在训练过程中,最优化(optimization)由三个步骤构成\n","\n","[1]``optimizer.zero_grad()`` 将model的梯度重置\n","梯度计算会不断积累,每次迭代都要重置\n","\n","[2]``loss.backwards()`` 向后传播\n","PyTorch对损失计算每个参数的梯度\n","\n","[3]``optimizer.step()``对各个参数的梯度进行超参数调整"],"metadata":{"id":"vkpmMoUOIPB8"}},{"cell_type":"markdown","source":["## **実装全体:Full Implementation**(实现)\n","\n","最优化实现的代码在``train_loop``中,使用test dataset对model性能评估的代码在``test_loop``中\n"],"metadata":{"id":"OhWZuuJ0JO7S"}},{"cell_type":"code","source":["def train_loop(dataloader, model, loss_fn, optimizer):\n","    size = len(dataloader.dataset)\n","\n","    # 迭代遍历数据\n","    # X是输入数据,y是标签\n","    for batch, (X, y) in enumerate(dataloader):\n","        # 预测值与损失的计算\n","        pred = model(X)\n","        loss = loss_fn(pred, y)\n","\n","        # 反向传播\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        # 每进行100批次,打印一次当前损失和训练进度\n","        if batch % 100 == 0:\n","            loss, current = loss.item(), batch * len(X)\n","            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n","\n","def test_loop(dataloader, model, loss_fn):\n","    size = len(dataloader.dataset)\n","    test_loss, correct = 0, 0\n","\n","    # 禁止梯度计算\n","    with torch.no_grad():\n","        for X, y in dataloader:\n","            pred = model(X)\n","            # 累加损失率\n","            test_loss += loss_fn(pred, y).item()\n","            # 累加正确率\n","            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n","            # pred.argmax(1) 获取每个样本的预测类别\n","            # == y 比较预测类别与实际类别，结果是一个布尔张量，然后将其转换为浮点型并求和。\n","\n","    # 计算平均损失率\n","    test_loss /= size\n","    # 计算平均正确率\n","    correct /= size\n","    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"],"metadata":{"id":"8EnZljMCJjEK","executionInfo":{"status":"ok","timestamp":1717570382722,"user_tz":-540,"elapsed":279,"user":{"displayName":"Moon Air","userId":"01277311785671261049"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# 声明loss function\n","loss_fn = nn.CrossEntropyLoss()\n","# 最优化\n","optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n","\n","# 迭代的次数(纪元数)\n","epochs = 20\n","\n","for t in range(epochs):\n","    print(f\"Epoch {t+1}\\n-------------------------------\")\n","    # 训练\n","    train_loop(train_dataloader, model, loss_fn, optimizer)\n","    # 测试\n","    test_loop(test_dataloader, model, loss_fn)\n","\n","print(\"Done!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xR5ubikqLIVm","executionInfo":{"status":"ok","timestamp":1717571046455,"user_tz":-540,"elapsed":299196,"user":{"displayName":"Moon Air","userId":"01277311785671261049"}},"outputId":"54cb9df0-89aa-4c72-8af3-679142fcc7ac"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1\n","-------------------------------\n","loss: 1.307315  [    0/60000]\n","loss: 1.310146  [ 6400/60000]\n","loss: 1.168489  [12800/60000]\n","loss: 1.209316  [19200/60000]\n","loss: 1.285666  [25600/60000]\n","loss: 1.405125  [32000/60000]\n","loss: 1.298121  [38400/60000]\n","loss: 1.470510  [44800/60000]\n","loss: 1.216217  [51200/60000]\n","loss: 1.224859  [57600/60000]\n","Test Error: \n"," Accuracy: 60.6%, Avg loss: 0.019474 \n","\n","Epoch 2\n","-------------------------------\n","loss: 1.265004  [    0/60000]\n","loss: 1.276382  [ 6400/60000]\n","loss: 1.130377  [12800/60000]\n","loss: 1.175241  [19200/60000]\n","loss: 1.256235  [25600/60000]\n","loss: 1.374448  [32000/60000]\n","loss: 1.267017  [38400/60000]\n","loss: 1.449112  [44800/60000]\n","loss: 1.189181  [51200/60000]\n","loss: 1.200372  [57600/60000]\n","Test Error: \n"," Accuracy: 61.2%, Avg loss: 0.019046 \n","\n","Epoch 3\n","-------------------------------\n","loss: 1.227385  [    0/60000]\n","loss: 1.246226  [ 6400/60000]\n","loss: 1.097373  [12800/60000]\n","loss: 1.146831  [19200/60000]\n","loss: 1.232689  [25600/60000]\n","loss: 1.347103  [32000/60000]\n","loss: 1.238845  [38400/60000]\n","loss: 1.430058  [44800/60000]\n","loss: 1.165421  [51200/60000]\n","loss: 1.177871  [57600/60000]\n","Test Error: \n"," Accuracy: 61.9%, Avg loss: 0.018670 \n","\n","Epoch 4\n","-------------------------------\n","loss: 1.192367  [    0/60000]\n","loss: 1.218898  [ 6400/60000]\n","loss: 1.068048  [12800/60000]\n","loss: 1.122608  [19200/60000]\n","loss: 1.212370  [25600/60000]\n","loss: 1.323763  [32000/60000]\n","loss: 1.213052  [38400/60000]\n","loss: 1.413566  [44800/60000]\n","loss: 1.143641  [51200/60000]\n","loss: 1.157637  [57600/60000]\n","Test Error: \n"," Accuracy: 62.3%, Avg loss: 0.018330 \n","\n","Epoch 5\n","-------------------------------\n","loss: 1.159454  [    0/60000]\n","loss: 1.193568  [ 6400/60000]\n","loss: 1.041262  [12800/60000]\n","loss: 1.101625  [19200/60000]\n","loss: 1.194855  [25600/60000]\n","loss: 1.303674  [32000/60000]\n","loss: 1.188575  [38400/60000]\n","loss: 1.398659  [44800/60000]\n","loss: 1.123587  [51200/60000]\n","loss: 1.139550  [57600/60000]\n","Test Error: \n"," Accuracy: 62.8%, Avg loss: 0.018018 \n","\n","Epoch 6\n","-------------------------------\n","loss: 1.128432  [    0/60000]\n","loss: 1.169357  [ 6400/60000]\n","loss: 1.016894  [12800/60000]\n","loss: 1.083261  [19200/60000]\n","loss: 1.179121  [25600/60000]\n","loss: 1.286137  [32000/60000]\n","loss: 1.165570  [38400/60000]\n","loss: 1.384659  [44800/60000]\n","loss: 1.105160  [51200/60000]\n","loss: 1.123006  [57600/60000]\n","Test Error: \n"," Accuracy: 63.5%, Avg loss: 0.017730 \n","\n","Epoch 7\n","-------------------------------\n","loss: 1.099215  [    0/60000]\n","loss: 1.146344  [ 6400/60000]\n","loss: 0.994438  [12800/60000]\n","loss: 1.066928  [19200/60000]\n","loss: 1.165239  [25600/60000]\n","loss: 1.270593  [32000/60000]\n","loss: 1.143690  [38400/60000]\n","loss: 1.372387  [44800/60000]\n","loss: 1.088453  [51200/60000]\n","loss: 1.107614  [57600/60000]\n","Test Error: \n"," Accuracy: 64.1%, Avg loss: 0.017462 \n","\n","Epoch 8\n","-------------------------------\n","loss: 1.071988  [    0/60000]\n","loss: 1.124681  [ 6400/60000]\n","loss: 0.973631  [12800/60000]\n","loss: 1.052146  [19200/60000]\n","loss: 1.151566  [25600/60000]\n","loss: 1.257184  [32000/60000]\n","loss: 1.123206  [38400/60000]\n","loss: 1.361504  [44800/60000]\n","loss: 1.073356  [51200/60000]\n","loss: 1.093386  [57600/60000]\n","Test Error: \n"," Accuracy: 64.4%, Avg loss: 0.017216 \n","\n","Epoch 9\n","-------------------------------\n","loss: 1.046911  [    0/60000]\n","loss: 1.104560  [ 6400/60000]\n","loss: 0.954474  [12800/60000]\n","loss: 1.038565  [19200/60000]\n","loss: 1.138927  [25600/60000]\n","loss: 1.245196  [32000/60000]\n","loss: 1.104755  [38400/60000]\n","loss: 1.352513  [44800/60000]\n","loss: 1.059924  [51200/60000]\n","loss: 1.079692  [57600/60000]\n","Test Error: \n"," Accuracy: 64.8%, Avg loss: 0.016990 \n","\n","Epoch 10\n","-------------------------------\n","loss: 1.023947  [    0/60000]\n","loss: 1.085797  [ 6400/60000]\n","loss: 0.936994  [12800/60000]\n","loss: 1.026120  [19200/60000]\n","loss: 1.127860  [25600/60000]\n","loss: 1.235308  [32000/60000]\n","loss: 1.087982  [38400/60000]\n","loss: 1.345141  [44800/60000]\n","loss: 1.047992  [51200/60000]\n","loss: 1.067273  [57600/60000]\n","Test Error: \n"," Accuracy: 65.1%, Avg loss: 0.016783 \n","\n","Epoch 11\n","-------------------------------\n","loss: 1.003046  [    0/60000]\n","loss: 1.068932  [ 6400/60000]\n","loss: 0.920944  [12800/60000]\n","loss: 1.015196  [19200/60000]\n","loss: 1.117893  [25600/60000]\n","loss: 1.226212  [32000/60000]\n","loss: 1.072818  [38400/60000]\n","loss: 1.338878  [44800/60000]\n","loss: 1.037600  [51200/60000]\n","loss: 1.055827  [57600/60000]\n","Test Error: \n"," Accuracy: 65.4%, Avg loss: 0.016594 \n","\n","Epoch 12\n","-------------------------------\n","loss: 0.984540  [    0/60000]\n","loss: 1.053360  [ 6400/60000]\n","loss: 0.906326  [12800/60000]\n","loss: 1.004882  [19200/60000]\n","loss: 1.108488  [25600/60000]\n","loss: 1.218122  [32000/60000]\n","loss: 1.059030  [38400/60000]\n","loss: 1.333497  [44800/60000]\n","loss: 1.028605  [51200/60000]\n","loss: 1.045359  [57600/60000]\n","Test Error: \n"," Accuracy: 65.7%, Avg loss: 0.016421 \n","\n","Epoch 13\n","-------------------------------\n","loss: 0.967906  [    0/60000]\n","loss: 1.039216  [ 6400/60000]\n","loss: 0.892732  [12800/60000]\n","loss: 0.995155  [19200/60000]\n","loss: 1.100067  [25600/60000]\n","loss: 1.210392  [32000/60000]\n","loss: 1.046459  [38400/60000]\n","loss: 1.328927  [44800/60000]\n","loss: 1.020837  [51200/60000]\n","loss: 1.035622  [57600/60000]\n","Test Error: \n"," Accuracy: 65.9%, Avg loss: 0.016264 \n","\n","Epoch 14\n","-------------------------------\n","loss: 0.952790  [    0/60000]\n","loss: 1.026469  [ 6400/60000]\n","loss: 0.880260  [12800/60000]\n","loss: 0.985951  [19200/60000]\n","loss: 1.091973  [25600/60000]\n","loss: 1.203674  [32000/60000]\n","loss: 1.035227  [38400/60000]\n","loss: 1.325242  [44800/60000]\n","loss: 1.014365  [51200/60000]\n","loss: 1.026534  [57600/60000]\n","Test Error: \n"," Accuracy: 66.1%, Avg loss: 0.016121 \n","\n","Epoch 15\n","-------------------------------\n","loss: 0.939076  [    0/60000]\n","loss: 1.015237  [ 6400/60000]\n","loss: 0.868911  [12800/60000]\n","loss: 0.977352  [19200/60000]\n","loss: 1.084800  [25600/60000]\n","loss: 1.196994  [32000/60000]\n","loss: 1.024863  [38400/60000]\n","loss: 1.321651  [44800/60000]\n","loss: 1.007456  [51200/60000]\n","loss: 1.018096  [57600/60000]\n","Test Error: \n"," Accuracy: 66.4%, Avg loss: 0.015990 \n","\n","Epoch 16\n","-------------------------------\n","loss: 0.926399  [    0/60000]\n","loss: 1.004970  [ 6400/60000]\n","loss: 0.858408  [12800/60000]\n","loss: 0.969314  [19200/60000]\n","loss: 1.078236  [25600/60000]\n","loss: 1.191165  [32000/60000]\n","loss: 1.015312  [38400/60000]\n","loss: 1.318499  [44800/60000]\n","loss: 1.001700  [51200/60000]\n","loss: 1.010283  [57600/60000]\n","Test Error: \n"," Accuracy: 66.5%, Avg loss: 0.015869 \n","\n","Epoch 17\n","-------------------------------\n","loss: 0.914867  [    0/60000]\n","loss: 0.995920  [ 6400/60000]\n","loss: 0.848709  [12800/60000]\n","loss: 0.961613  [19200/60000]\n","loss: 1.072353  [25600/60000]\n","loss: 1.185560  [32000/60000]\n","loss: 1.006643  [38400/60000]\n","loss: 1.315117  [44800/60000]\n","loss: 0.996428  [51200/60000]\n","loss: 1.003018  [57600/60000]\n","Test Error: \n"," Accuracy: 66.6%, Avg loss: 0.015759 \n","\n","Epoch 18\n","-------------------------------\n","loss: 0.904321  [    0/60000]\n","loss: 0.987944  [ 6400/60000]\n","loss: 0.839772  [12800/60000]\n","loss: 0.954401  [19200/60000]\n","loss: 1.066362  [25600/60000]\n","loss: 1.180187  [32000/60000]\n","loss: 0.998705  [38400/60000]\n","loss: 1.312264  [44800/60000]\n","loss: 0.991898  [51200/60000]\n","loss: 0.996171  [57600/60000]\n","Test Error: \n"," Accuracy: 66.8%, Avg loss: 0.015657 \n","\n","Epoch 19\n","-------------------------------\n","loss: 0.894351  [    0/60000]\n","loss: 0.980854  [ 6400/60000]\n","loss: 0.831478  [12800/60000]\n","loss: 0.947590  [19200/60000]\n","loss: 1.060202  [25600/60000]\n","loss: 1.174692  [32000/60000]\n","loss: 0.991380  [38400/60000]\n","loss: 1.309980  [44800/60000]\n","loss: 0.987806  [51200/60000]\n","loss: 0.989344  [57600/60000]\n","Test Error: \n"," Accuracy: 66.9%, Avg loss: 0.015563 \n","\n","Epoch 20\n","-------------------------------\n","loss: 0.885088  [    0/60000]\n","loss: 0.974598  [ 6400/60000]\n","loss: 0.823987  [12800/60000]\n","loss: 0.940974  [19200/60000]\n","loss: 1.054668  [25600/60000]\n","loss: 1.168965  [32000/60000]\n","loss: 0.984994  [38400/60000]\n","loss: 1.308281  [44800/60000]\n","loss: 0.983855  [51200/60000]\n","loss: 0.982784  [57600/60000]\n","Test Error: \n"," Accuracy: 67.0%, Avg loss: 0.015475 \n","\n","Done!\n"]}]}]}